# -*- coding: utf-8 -*-
"""Copy of Data Science project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mbCUCSSPNrD5fKYpW2xRe5Ue0Lmd8d1Q
"""



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv"
data = pd.read_csv(url)
data.drop(["Browser", "OperatingSystems", "Region","SpecialDay"], axis=1, inplace=True)

column_names = data.columns.values.tolist()
data_for_visualization = column_names[0:12]
print(data.head())

print(data.shape)

print(data.describe())

data_type = data.dtypes

data.corr().round(2)

#Correlation of revenue with all columns
data[data.columns[:]].corr()['Revenue'][:-1]

cmap = sns.diverging_palette(220,10,as_cmap=True)
fig, ax = plt.subplots(figsize=(8,8))
corr =  data.corr().round(2)

# Create a mask
mask = np.triu(np.ones_like(corr, dtype=bool))
np.fill_diagonal(mask, False)

dataplot = sns.heatmap(corr,mask=mask, cmap='jet', annot=True,vmin=-1, vmax=1,linewidth=.5,square= True,cbar_kws={"extend": 'both'},ax=ax)
plt.show()

#Plotting numeric columns

numeric_cols = data.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns
categorical_cols = data.select_dtypes(include=["bool_","object_"]).columns

for col in numeric_cols:
    data[col].hist()
    plt.title(col)
    plt.show()

data['BounceRates'].mean()

data['ExitRates'].mean()

#Bounce rates and exit rates are pretty low which suggests there is some issues with the site tracking these values.
#The range for e-commerce websites usually go from 20-45%. But in our case the mean is coming to 2% and 4% which is very low.
#There are just a few observations with 20% bounce and exit rates.

#People don't like shopping during the weekends, this data follows the various previous observations from different datas around the world.

# Turkey has 2 festivals back to back in may (Hidirellez Spring Celebration and Eid) which explains the highest shopping rates.
# There are no big events or religious festivals that might explain the sales in november, but we can consider this spike as people shopping for winter.

for col in numeric_cols:
    data.boxplot(column= col,showfliers=False)
    plt.title(col)
    plt.show()

#Plotting categorical data
df_categorical = data[['Month', 'VisitorType', 'Weekend', 'Revenue']]
for i in df_categorical:
  num = df_categorical[i].value_counts()
  sns.barplot(x=num.index,y=num)
  plt.title(i)
  plt.show()

pd.pivot_table(data, index='Month',aggfunc={'ProductRelated_Duration': 'mean'})

#The above table shows mean of the time spent on product related pages for the month.
#Interestingly May even after being the month with highest revenue generated, people are not spending as much time on product page during the month.
#As you can see from the table below, the amount of time spent on revenue being generated comes to 1876 on average which is very less compared to the time spent on product pages during may.
#Unfortunately we don't have data for april, but there is a high possibility people have spent more time on product pages during april and put products in the shopping in april itself.

pd.pivot_table(data, index='Revenue',values="ProductRelated_Duration")

data['time_spent_on_1_product_page'] = data['ProductRelated_Duration']/data['ProductRelated']
data['DurationBin_per_product'] = pd.cut(data['time_spent_on_1_product_page'], bins=range(0,150, 5))

bin_counts = data['DurationBin_per_product'].value_counts().sort_index()
fig, ax = plt.subplots(figsize=(20, 20))
plt.bar(bin_counts.index.astype(str), bin_counts.values)
plt.title('Distribution of time spent on single a product page')
plt.xlabel('Duration Bins(min)')
plt.ylabel('Frequency')

for i, v in enumerate(bin_counts.values):
  ax.text(i, v, str(v), ha='center', va='bottom')

plt.show()

pd.pivot_table(data, index=['Revenue','VisitorType'],values="ProductRelated_Duration")

pd.pivot_table(data, index='Revenue',columns="Month",values='ProductRelated_Duration')

# create a new column with bins of 2 minute each
data['ProductRelated_Duration'] = data['ProductRelated_Duration'] / 60
data['DurationBin'] = pd.cut(data['ProductRelated_Duration'], bins=range(0, 40, 2))

bin_counts = data['DurationBin'].value_counts().sort_index()
fig, ax = plt.subplots(figsize=(16, 16))
plt.bar(bin_counts.index.astype(str), bin_counts.values)
plt.title('Distribution of ProductRelated_Duration')
plt.xlabel('Duration Bins(min)')
plt.ylabel('Frequency')

for i, v in enumerate(bin_counts.values):
  ax.text(i, v, str(v), ha='center', va='bottom')

plt.show()

X = data['ProductRelated_Duration'].values
y = data['Revenue'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#logistic regression model
model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(1,)),
  tf.keras.layers.Dense(1, activation='relu')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100, batch_size=32)

loss, accuracy = model.evaluate(X_test, y_test)
print("Accuracy:", accuracy)

X_train= X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)

forest_model = RandomForestClassifier(max_depth=50, random_state=0)
forest_model.fit(X_train, y_train)
y_pred = forest_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

